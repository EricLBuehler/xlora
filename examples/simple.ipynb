{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of X-LoRA API\n",
    "Eric Buehler 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xlora\n",
    "from transformers import AutoConfig, AutoModelForCausalLM  # type: ignore\n",
    "from xlora.xlora_utils import load_model  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    trust_remote_code=True,\n",
    "    use_flash_attention_2=False,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    trust_remote_code=True,\n",
    "    use_flash_attention_2=False,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the model to X-LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_created = xlora.add_xlora_to_model(\n",
    "    model=model,\n",
    "    xlora_config=xlora.xLoRAConfig(\n",
    "        config.hidden_size,\n",
    "        base_model_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        xlora_depth=8,\n",
    "        device=torch.device(\"cuda\"),\n",
    "        adapters={\n",
    "            \"adapter_1\": \"./path/to/the/checkpoint/\",\n",
    "            \"adapter_2\": \"./path/to/the/checkpoint/\",\n",
    "            \"adapter_n\": \"./path/to/the/checkpoint/\",\n",
    "        },\n",
    "    ),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the adapters to trainable\n",
    "This means that when loading the model again we should specify only the adapter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trainable adapters: mark all adapters as trainable\n",
    "model_created.set_use_trainable_adapters(True)\n",
    "\n",
    "# Get the current status of the trainable adapters, in this case returning True\n",
    "model_created.get_use_trainable_adapters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set and resetting the scaling pass value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the scaling pass value to 0, meaning that no adapters will contribute to the scaling pass output\n",
    "model_created.set_scaling_pass_value(0)\n",
    "\n",
    "# Allow the model to use the default scaling pass value\n",
    "model_created.set_scaling_pass_value(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting and getting the global LoRA weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the output of each LoRA adapter by 2, additionally to the scalings.\n",
    "model_created.set_global_scaling_weight(2)\n",
    "\n",
    "# Returns 2\n",
    "res = model_created.get_global_scaling_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of scalings logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable scalings logging and begin a log\n",
    "model_created.enable_scalings_logging()\n",
    "\n",
    "# Run forward passes to accumulate a log\n",
    "\n",
    "# Write the log to a file, or multiple.\n",
    "model_created.flush_log_scalings(\"./path/to/output/file\")\n",
    "\n",
    "# Get a shallow copy of the scalings\n",
    "log_copy = model_created.get_scalings_log()\n",
    "\n",
    "# Disable scalings logging\n",
    "model_created.disable_scalings_logging()\n",
    "\n",
    "# Clear the scalings log\n",
    "model_created.clear_scalings_log()\n",
    "\n",
    "# Get the latest scalings prediction\n",
    "scalings_pred = model_created.get_latest_scalings()\n",
    "\n",
    "# Load the scalings log from a file, or multiple automatically.\n",
    "loaded_log = xlora.xlora_utils.load_scalings_log(\"./path/to/output/file\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of getting, printing trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable, num_all_params = model_created.get_nb_trainable_parameters()\n",
    "\n",
    "model_created.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting and getting the top-k lora value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the top 2 lora experts\n",
    "model_created.set_topk_lora(2)\n",
    "\n",
    "# Returns 2\n",
    "res = model_created.get_topk_lora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_created = xlora.from_pretrained(\n",
    "    \"./path/to/saved/model\",\n",
    "    model,\n",
    "    \"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the simpler API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLoRA_model_name = \"myuser/repo\"\n",
    "\n",
    "model_loaded, tokenizer = load_model(\n",
    "    model_name=XLoRA_model_name,\n",
    "    device=\"cuda:0\",\n",
    "    dtype=torch.bfloat16,\n",
    "    adapters={\n",
    "        \"adapter_1\": \"./path/to/the/checkpoint/\",\n",
    "        \"adapter_2\": \"./path/to/the/checkpoint/\",\n",
    "        \"adapter_n\": \"./path/to/the/checkpoint/\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
