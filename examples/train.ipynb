{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3288987d",
   "metadata": {},
   "source": [
    "# MoLE training\n",
    "\n",
    "Multifile dataset augmentation and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8274a027-6bee-4a0b-995e-30fbe883ae4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c18b20-b1a9-4f3e-ae84-2a551e2ed69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "def generate_response (text_input=\"Biology offers amazing\",\n",
    "                      num_return_sequences=1,\n",
    "                      temperature=1., #the higher the temperature, the more creative the model becomes\n",
    "                      max_new_tokens=127,\n",
    "                      num_beams=1,\n",
    "                      top_k = 50,\n",
    "                      top_p =0.9,repetition_penalty=1.,eos_token_id=2,verbatim=False,\n",
    "                      exponential_decay_length_penalty_fac=None,\n",
    "                      ):\n",
    "\n",
    "    inputs = tokenizer.encode(text_input,  \n",
    "                              add_special_tokens  =False,  \n",
    "                              return_tensors ='pt')\n",
    "    if verbatim:\n",
    "        print (\"Length of input, tokenized: \", inputs.shape, inputs)\n",
    "    with torch.no_grad():\n",
    "          print(inputs.shape)\n",
    "          outputs = model.generate(input_ids=inputs.to(device), \n",
    "                                   max_new_tokens=max_new_tokens,\n",
    "                                   temperature=temperature, #value used to modulate the next token probabilities.\n",
    "                                   num_beams=num_beams,\n",
    "                                   top_k = top_k,\n",
    "                                   top_p =top_p,\n",
    "                                   num_return_sequences = num_return_sequences, eos_token_id=eos_token_id,\n",
    "                                   do_sample =True,#skip_prompt=True,\n",
    "                                   # exponential_decay_length_penalty = [inputs.shape[1], \n",
    "                                   #                                     exponential_decay_length_penalty_fac],\n",
    "                                   # length_penalty =   exponential_decay_length_penalty_fac,\n",
    "                                    \n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                                   #repetition_penalty_range = 2048,\n",
    "                                    #    repetition_penalty_slope = 0,\n",
    "                                  )\n",
    "    return tokenizer.batch_decode(outputs[:,inputs.shape[1]:].detach().cpu().numpy(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea280c0-226f-46d9-875a-181dee3e99ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "on_Lambda = True\n",
    "\n",
    "if on_Lambda:\n",
    "    !python3 -m pip install --upgrade pip\n",
    "    \n",
    "    !pip install -U trl git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/accelerate.git git+https://github.com/huggingface/peft.git\n",
    "    !pip install -U datasets einops wandb unidecode protobuf==3.20.0 bitsandbytes \n",
    "    !pip install -U trl\n",
    "    !pip install -U scipy\n",
    "    !pip install -U matplotlib\n",
    "    \n",
    "else:\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ['HUGGINGFACE_HUB_CACHE '] = \"/mnt/d/.cache_Huggingface/\"\n",
    "    os.environ['HF_HOME'] = \"/mnt/d/.cache_Huggingface/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d128e116-685c-43c6-975e-5aba54c7a01d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from datasets import IterableDataset\n",
    "\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainerCallback\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b952f32a-8d45-478f-b2b5-3dd43a80dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e223522c-23bd-4d6e-9fdb-7b760b478f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('12.1', '2.1.2', 8902, True, True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda, torch.__version__, torch.backends.cudnn.version(), torch.backends.cudnn.enabled, torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1065c76-f2fd-4dee-ba77-a4703284fa02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NVIDIA RTX A6000', 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8a59e4-5a44-4b7f-82ef-0d242238bcfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1493b78-cf71-4908-8f02-bf4973ade4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2acf1c-3f42-49a1-a238-2ba095279fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token = 'hf_xxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "from huggingface_hub import login\n",
    "#login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de85a04d-6503-49cc-ac21-8bb42386324e",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82229f69-0acc-4dae-b4f9-4fa570c13153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "learning_rate = 1e-4\n",
    "#warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "batch_size=1 \n",
    "train_test_split = 1 #80% of the FILES\n",
    "gradient_accumulation_steps=1\n",
    "\n",
    "output_dir = 'Mistral_v200Zephyr'\n",
    "FT_model_name = 'Mistral_v200Zephyr' #BioLlama_7b_4bit_190_withend\n",
    "loss_file='Mistral_v200Zephyr.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da50a16-185d-4ff9-9580-6eb5486ca0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a8691a5-197d-48e2-b60c-f2554db3cb2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "    print(\"Parameters: \", params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d4eb2b3-b185-4916-91c3-59e73456a718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70ec3cd-23e6-481e-8385-ec51fabc2f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e0537b-a55f-4bf9-b0e1-0a1ae951fb68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fdf53d76a34cdcbb3a806f7ef9f216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name= 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "bnb_config4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    #bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    use_nested_quant = False,\n",
    ")\n",
    "\n",
    "bnb_config8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    #bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #quantization_config= bnb_config8bit, \n",
    "    #quantization_config= bnb_config4bit, #bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    #load_in_8bit=True, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=  torch.bfloat16,\n",
    "   # use_flash_attention_2=True,\n",
    "    #device_map=\"cuda:0\",\n",
    ")#.to (device)\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9bf521-a1d9-4bb8-892c-80b5cdca7900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47823701-01ba-478d-9f1e-3ae0a8471c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name= 'HuggingFaceH4/zephyr-7b-beta'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                         #device_map=\"cuda:0\",\n",
    "                                          device_map=\"auto\",\n",
    "                                         )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token\n",
    "#tokenizer.padding_side = \"left\" # Fix weird overflow issue with fp16 training\n",
    "eos_token= tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1275be8c-5deb-4432-aec1-d1a7a5094dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d851a462-3a81-42db-8231-f2acadf951a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nYou are a friendly chatbot who always responds in the style of a pirate</s>\\n<|user|>\\nHow many helicopters can a human eat in one sitting?</s>\\n<|assistant|>\\nSample response.</s>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sample response.\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6883e74d-c13c-4bb2-847f-b943c7aea790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/mole'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5785ac-f3f3-44df-8a17-3d1979b8cd99",
   "metadata": {},
   "source": [
    "### Set up LoRA model (adaptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e15f67d3-dd2b-4352-be56-e9ddba845deb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2376/2376 [00:00<00:00, 1033032.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swapped 160 layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "import mole\n",
    " \n",
    "from importlib import reload\n",
    "import torch\n",
    "\n",
    "mole=reload(mole)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = mole.add_mole_to_model(model = model,\n",
    "                              \n",
    "                       mole_config = mole.MoLEConfig(config.hidden_size, mole_depth=2,\n",
    "                                pad_token_id=tokenizer.pad_token_id, device=torch.device(\"cuda\")),\n",
    "                     verbose=True,\n",
    "                       adapters = {\"adapter_1\": \"./mole-checkpoints/checkpoint-a\", \n",
    "                                   \"adapter_2\": \"./mole-checkpoints/checkpoint-a\",\n",
    "                                  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ae9964c-f587-487c-98d8-336a7082dc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a105bbea-d6e5-45cb-8527-3c498100db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  0\n"
     ]
    }
   ],
   "source": [
    "params (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ed32a59-561e-4fec-aeb3-18683c630b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 131,136 || all params: 14,575,870,016 || trainable%: 0.0009\n"
     ]
    }
   ],
   "source": [
    "mole.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cd9e05f-0d67-404f-bee9-1cc7500ef19d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                  (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  (adapter_2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                  (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  (adapter_2): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                  (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  (adapter_2): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                  (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  (adapter_2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                  (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (adapter_1): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                  (adapter_2): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ed55259-9314-4f07-9769-d8aa609726a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "make_peft=True\n",
    "make_peft=False\n",
    "if make_peft:\n",
    "    peft_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]\n",
    "        )\n",
    "    \n",
    "    model=get_peft_model(model, peft_config)\n",
    "    params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a58b033-da3f-454e-9057-21fdf4c718f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 131,136 || all params: 14,575,870,016 || trainable%: 0.0009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PeftModelForCausalLM(\n",
       "   (base_model): LoraModel(\n",
       "     (model): MistralForCausalLM(\n",
       "       (model): MistralModel(\n",
       "         (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "         (layers): ModuleList(\n",
       "           (0-31): 32 x MistralDecoderLayer(\n",
       "             (self_attn): MistralSdpaAttention(\n",
       "               (q_proj): lora.Linear(\n",
       "                 (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                 (lora_dropout): ModuleDict(\n",
       "                   (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                   (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                 )\n",
       "                 (lora_A): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                   (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                 )\n",
       "                 (lora_B): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                   (adapter_2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                 )\n",
       "                 (lora_embedding_A): ParameterDict()\n",
       "                 (lora_embedding_B): ParameterDict()\n",
       "               )\n",
       "               (k_proj): lora.Linear(\n",
       "                 (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                 (lora_dropout): ModuleDict(\n",
       "                   (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                   (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                 )\n",
       "                 (lora_A): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                   (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                 )\n",
       "                 (lora_B): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                   (adapter_2): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                 )\n",
       "                 (lora_embedding_A): ParameterDict()\n",
       "                 (lora_embedding_B): ParameterDict()\n",
       "               )\n",
       "               (v_proj): lora.Linear(\n",
       "                 (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                 (lora_dropout): ModuleDict(\n",
       "                   (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                   (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                 )\n",
       "                 (lora_A): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                   (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                 )\n",
       "                 (lora_B): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                   (adapter_2): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                 )\n",
       "                 (lora_embedding_A): ParameterDict()\n",
       "                 (lora_embedding_B): ParameterDict()\n",
       "               )\n",
       "               (o_proj): lora.Linear(\n",
       "                 (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                 (lora_dropout): ModuleDict(\n",
       "                   (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                   (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                 )\n",
       "                 (lora_A): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                   (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                 )\n",
       "                 (lora_B): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                   (adapter_2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                 )\n",
       "                 (lora_embedding_A): ParameterDict()\n",
       "                 (lora_embedding_B): ParameterDict()\n",
       "               )\n",
       "               (rotary_emb): MistralRotaryEmbedding()\n",
       "             )\n",
       "             (mlp): MistralMLP(\n",
       "               (gate_proj): lora.Linear(\n",
       "                 (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                 (lora_dropout): ModuleDict(\n",
       "                   (adapter_1): Dropout(p=0.05, inplace=False)\n",
       "                   (adapter_2): Dropout(p=0.05, inplace=False)\n",
       "                 )\n",
       "                 (lora_A): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                   (adapter_2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                 )\n",
       "                 (lora_B): ModuleDict(\n",
       "                   (adapter_1): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                   (adapter_2): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                 )\n",
       "                 (lora_embedding_A): ParameterDict()\n",
       "                 (lora_embedding_B): ParameterDict()\n",
       "               )\n",
       "               (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "               (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "               (act_fn): SiLU()\n",
       "             )\n",
       "             (input_layernorm): MistralRMSNorm()\n",
       "             (post_attention_layernorm): MistralRMSNorm()\n",
       "           )\n",
       "         )\n",
       "         (norm): MistralRMSNorm()\n",
       "       )\n",
       "       (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, mole.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c48f2ffa-a2b4-4bd5-bcc8-d0e57e926fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MoLEConfig', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'add_mole_to_model', 'from_pretrained', 'get_nb_trainable_parameters', 'mole', 'mole_classifier', 'mole_config', 'mole_insertion', 'mole_state', 'print_trainable_parameters', 'save_pretrained', 'set_scalings_with_lifetime']\n"
     ]
    }
   ],
   "source": [
    "print (dir(mole))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c2bbfb3-2551-4921-8a7b-57aacbe53750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.add_bos_token=True #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "231d38b5-25a6-4076-8e85-f0887b987c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "fname='./BioMat_ExaminationsQ_UPDATED.csv'\n",
    "df_trial=pd.read_csv(fname)\n",
    "df_trial\n",
    "\n",
    "eos_token=tokenizer.eos_token_id\n",
    "\n",
    "def trial(df_trial, step_number=0, frac=1. ):\n",
    "\n",
    "    questions=[]\n",
    "    answers=[]\n",
    "    corr_answers=[]\n",
    "\n",
    "    if frac<1:\n",
    "        df_trial=df_trial.sample (frac=frac, random_state=42).reset_index()\n",
    "    \n",
    "    for q,CorrectA in tqdm(zip(df_trial['Question'],df_trial['Answer'])):    \n",
    "        print (\"###########################################################################\")\n",
    "        #q=f'Select only one option as the correct answer (A,  B, or C): \"{q.strip()}\"'\n",
    "        #q=f'{q.strip()}'\n",
    "        \n",
    "        #txt=f\"<s>### System:\\n{system_prompt}\\n\\n### User:\\n{q}\\n\\n### Assistant:\\n{prepend_response}\" \n",
    "        \n",
    "        #txt=system_prompt+f\"Select only one option as the correct answer (A,  B, or C): \" + q + \"[/INST]\"\n",
    "        #txt=f\"[INST] {q} [/INST]\"\n",
    "        #txt=f\"<s>{system_prompt} [INST] Select only one option as the correct answer (A,  B, or C): \" + q + \"[/INST]\"\n",
    "        #txt=f\"<s>[INST] Select only one option as the correct answer (A,  B, or C): \" + q + \"[/INST]\"\n",
    "        #txt=f\"<s>Select the correct answer</s><s>. [INST] \" + q + \"[/INST]\"\n",
    "        #txt=f\"<s>You are an expert in biological and bio-inspired materials. [INST] Select only one option as the correct answer (A,  B, or C): \" + q + \". Very brief answer. [/INST]\"\n",
    "        txt=f\"<s><|user|>\\nSelect only one option as the correct answer (A,  B, or C): \" + q.strip() + \"</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "        #'<|system|>\\nYou are a friendly chatbot who always responds in the style of a pirate</s>\\n<|user|>\\nHow many helicopters \n",
    "        #can a human eat in one sitting?</s>\\n<|assistant|>\\n\n",
    "\n",
    "        \n",
    "        #txt=f\"<s>You are an expert in biological and bio-inspired materials. [INST] Read this question: \"+ q + \"Select only one option as the correct answer (A,  B, or C): \" + q + \"[/INST]\"\n",
    "        #txt=f\"<s> Select only one option as the correct answer (A,  B, or C): \" + q  \n",
    "        #print (f\"QUESTION: {txt}\")\n",
    "        #txt=f\"<s>{q}\"\n",
    "        #txt=f\"<s>### User:\\n{q}\\n\\n### Assistant:\\n{prepend_response}\" \n",
    "        #print (txt) \n",
    "        #result=generate_response (\"[INST] Select only one option as the correct answer (A,  B, or C): \" + csv_data_list[l] + \"[/INST]\",repetition_penalty = 1.1, \\\n",
    "        #temperature=.1,max_new_tokens=100,num_return_sequences=5) \n",
    "    \n",
    "        \n",
    "        output_text=generate_response (text_input=txt,eos_token_id=eos_token,\n",
    "                              num_return_sequences=1,  repetition_penalty=1.,\n",
    "                                       # top_p=0.95, top_k=500, \n",
    "                                       top_p=0.9, top_k=50,  \n",
    "                             #temperature=.1,max_new_tokens=100, verbatim=True, \n",
    "                             temperature=.1,max_new_tokens=256, verbatim=False, \n",
    "                                        \n",
    "                                       )\n",
    "      \n",
    "        \n",
    "        #print(\"\\nANSWER: \", output_text[0],'\\n')\n",
    "        print (f\"Question: {q}\\nAnswer: {output_text[0]}\\nCorrect answer={CorrectA}\")\n",
    "        questions.append (q)\n",
    "        answers.append (output_text[0])\n",
    "        corr_answers.append (CorrectA)\n",
    "\n",
    "    \n",
    "    df_res = pd.DataFrame({\"text\": questions, \"answers\": answers, \"corr_answers\": corr_answers} )\n",
    "    df_res.to_csv(f'./{output_dir}/out_{FT_model_name}_{step_number}.csv')\n",
    "\n",
    "    return df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27f6b128-abfb-41cd-9609-fc7333fa1afd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde066f-b921-4ee4-968f-8f038ef1baba",
   "metadata": {},
   "source": [
    "### Data loader and related functions "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e01ddd94-3a3b-48e2-b85d-76ec4745ad09",
   "metadata": {},
   "source": [
    "dataset = load_dataset('json', data_files=\"./CoT_data.json\", )\n",
    "\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    display (item['instruction'],item['output'], )\n",
    "\n",
    "    messages = [\n",
    "         \n",
    "        {\"role\": \"user\", \"content\": item['instruction']},\n",
    "        {\"role\": \"assistant\", \"content\": item['output']},\n",
    "        \n",
    "        ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    display (prompt)\n",
    "        \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0662a824-9748-4d4c-8239-6fab400b1621",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aff124d682f4365ab69d3aa8714600f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokenized: 511, max length (words): 386\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ALTERNATIVE DATALOADER\n",
    "\n",
    "    \n",
    "#start_token = '<s>'\n",
    "#end_token = '</s>'\n",
    "#sys_token = '<<SYS>>'\n",
    "#or_start_token = '[INST]'\n",
    "#or_end_token = '[/INST]' \n",
    "\n",
    "pad_all=False #then padding happens in data collector \n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_list):\n",
    "        #self.titles = df['Article Title']\n",
    "        #self.abstracts = df['Abstract']\n",
    "        self.textlist=text_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         \n",
    "        #line =  f\"{inst_start_token} {self.titles[idx]} {inst_end_token} {self.abstracts[i]} {end_token}\"\n",
    "        line =  self.textlist[idx]\n",
    "        #print (line[:16])\n",
    "        #print (f'{idx}-', end='')\n",
    "        \n",
    "        if text_only:\n",
    "            out = {\n",
    "                   \"text\": line\n",
    "                  }\n",
    "        \n",
    "        elif pad_all:\n",
    "            res = tokenizer(line, truncation=True, max_length=MAX_TOKENIZED_LENGTH, \n",
    "                                    padding=\"max_length\",\n",
    "                                    return_tensors=\"pt\",\n",
    "                                   \n",
    "                                   )\n",
    "\n",
    "            out = {\n",
    "                   \"input_ids\": res[\"input_ids\"].squeeze(), \"attention_mask\":res[\"attention_mask\"].squeeze(),\n",
    "                   \"labels\": res[\"input_ids\"].clone().detach().squeeze()#,\n",
    "                   #\"text\": line\n",
    "                  }\n",
    "        \n",
    "        else:\n",
    "            res = tokenizer(line, #truncation=True, max_length=MAX_TOKENIZED_LENGTH, \n",
    "                                   # padding=\"max_length\",\n",
    "                                    return_tensors=\"pt\",\n",
    "                                   \n",
    "                                   )\n",
    "    \n",
    "            out = {\n",
    "                   \"input_ids\": res[\"input_ids\"].squeeze(), \"attention_mask\":res[\"attention_mask\"].squeeze(),\n",
    "                   \"labels\": res[\"input_ids\"].clone().detach().squeeze()#,\n",
    "                   #\"labels\": res[\"input_ids\"].clone().detach().squeeze(),\n",
    "                   #\"text\": line\n",
    "                  }\n",
    "        \n",
    "        #print (item)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.textlist)\n",
    "\n",
    "\n",
    "\n",
    "max_leng_critical=1800\n",
    "max_leng_critical=512\n",
    "\n",
    "max_len=-1\n",
    "max_len_txt=-1\n",
    "len_list=[]\n",
    "len_list_txt=[]\n",
    "text_list=[]\n",
    "\n",
    "use13b=True\n",
    "Orca_include=True\n",
    "Orca_singleout_include=True\n",
    "#numerical_include=False\n",
    "numerical_include=True\n",
    "\n",
    "use13b=False\n",
    "Orca_include=False\n",
    "Orca_singleout_include=False\n",
    "#numerical_include=False\n",
    "numerical_include=False\n",
    "\n",
    "CoT_include=False #True\n",
    "\n",
    "Q_A_include=True\n",
    "format_qa=True #whether to use chat template \n",
    "\n",
    "if Q_A_include:\n",
    "    \n",
    "    data=pd.read_csv('train_data_processed_Mistral_train_data_small_COMBINED.csv_0_FINAL.csv')\n",
    "    \n",
    "    for idx in tqdm ( range (len (data['question'])) ):\n",
    "        \n",
    "        #system_prompt='You are BioinspiredLLM, a helpful assistant.'\n",
    "        \n",
    "        if format_qa:\n",
    "\n",
    "            messages = [\n",
    "             \n",
    "            {\"role\": \"user\", \"content\": data['question'][idx]},\n",
    "            {\"role\": \"assistant\", \"content\": data['answer'][idx]},\n",
    "            \n",
    "            ]\n",
    "            line = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "        else:\n",
    "            line=f\"{data['question'][idx]} {data['answer'][idx]}</s>\" \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        tok=tokenizer(line,  return_tensors=\"pt\",  )\n",
    "    \n",
    "        if tok['input_ids'].shape[1]<max_leng_critical:\n",
    "            \n",
    "            if tok['input_ids'].shape[1]>=max_len:\n",
    "                max_len=tok['input_ids'].shape[1]\n",
    "        \n",
    "                \n",
    "                #print (len (line.split ()))\n",
    "            if len (line.split ())>max_len_txt:\n",
    "                max_len_txt=len (line.split ())\n",
    "        \n",
    "            len_list.append (tok['input_ids'].shape[1])\n",
    "            len_list_txt.append (len (line.split ()))\n",
    "        \n",
    "            text_list.append (line)\n",
    "\n",
    "\n",
    "\n",
    "if CoT_include:\n",
    "    max_leng_critical_CoT=1000\n",
    "    \n",
    "    data=pd.read_csv('ALLDATASETS/13b_train_data_smallv2_processed_llama_B_MERGED.csv')\n",
    "\n",
    "    dataset = load_dataset('json', data_files=\"./CoT_data.json\", )\n",
    "\n",
    "\n",
    "    for item in tqdm (dataset[\"train\"]):\n",
    "        \n",
    "        messages = [\n",
    "         \n",
    "        {\"role\": \"user\", \"content\": item['instruction']},\n",
    "        {\"role\": \"assistant\", \"content\": item['output']},\n",
    "        \n",
    "        ]\n",
    "        line = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    \n",
    "        tok=tokenizer(line,  return_tensors=\"pt\",  )\n",
    "    \n",
    "        if tok['input_ids'].shape[1]<max_leng_critical_CoT:\n",
    "            \n",
    "            if tok['input_ids'].shape[1]>=max_len:\n",
    "                max_len=tok['input_ids'].shape[1]\n",
    "        \n",
    "                \n",
    "                #print (len (line.split ()))\n",
    "            if len (line.split ())>max_len_txt:\n",
    "                max_len_txt=len (line.split ())\n",
    "        \n",
    "            len_list.append (tok['input_ids'].shape[1])\n",
    "            len_list_txt.append (len (line.split ()))\n",
    "        \n",
    "            text_list.append (line)\n",
    "\n",
    "\n",
    "if use13b:\n",
    "    \n",
    "    data=pd.read_csv('../ALLDATASETS/13b_train_data_smallv2_processed_llama_B_MERGED.csv')\n",
    "    \n",
    "    for idx in tqdm ( range (len (data['question'])) ):\n",
    "        '''\n",
    "        <s>[INST] <<SYS>>\n",
    "        You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "        \n",
    "        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "        <</SYS>>\n",
    "        \n",
    "        There's a llama in my garden 😱 What should I do? [/INST]\n",
    "        '''\n",
    "        system_prompt='You are BioinspiredLLM, a helpful assistant.'\n",
    "        ########################################################\n",
    "        #line=f\"{CNT['text'][idx]}{end_token}\"\n",
    "        #line=f\"{or_start_token} {CNT['question'][idx]} {or_end_token} {CNT['answer'][idx]} {end_token}\" \n",
    "        #line=f\"<|user|>\\n{data['question'][idx]}</s>\\n<|assistant|>\\n{data['answer'][idx]}</s>\" \n",
    "        line=f\"{data['question'][idx]} {data['answer'][idx]}</s>\" \n",
    "        #prompt =  f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "    \n",
    "    \n",
    "    \n",
    "        #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "        #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "    \n",
    "        #line=f\"{data['question'][idx]} {data['answer'][idx]}\" \n",
    "        tok=tokenizer(line,  return_tensors=\"pt\",  )\n",
    "    \n",
    "        if tok['input_ids'].shape[1]<max_leng_critical:\n",
    "            \n",
    "            if tok['input_ids'].shape[1]>=max_len:\n",
    "                max_len=tok['input_ids'].shape[1]\n",
    "        \n",
    "                \n",
    "                #print (len (line.split ()))\n",
    "            if len (line.split ())>max_len_txt:\n",
    "                max_len_txt=len (line.split ())\n",
    "        \n",
    "            len_list.append (tok['input_ids'].shape[1])\n",
    "            len_list_txt.append (len (line.split ()))\n",
    "        \n",
    "            text_list.append (line)\n",
    "\n",
    "\n",
    "if Orca_singleout_include:\n",
    "    fname='ALLDATASETS/train_data_processed_OpenOrca_singleoutput_train_data_chunked_text.csv_0_Lambda_0_FINAL.csv'\n",
    "    data=pd.read_csv(fname)\n",
    "    #data.head ()\n",
    "    for idx in tqdm ( range (len (data['title'])) ):\n",
    "        '''\n",
    "        <s>[INST] <<SYS>>\n",
    "        You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "        \n",
    "        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "        <</SYS>>\n",
    "        \n",
    "        There's a llama in my garden 😱 What should I do? [/INST]\n",
    "        '''\n",
    "        system_prompt='You are BioinspiredLLM, a helpful assistant.'\n",
    "        ########################################################\n",
    "        #line=f\"{CNT['text'][idx]}{end_token}\"\n",
    "        #line=f\"{or_start_token} {CNT['question'][idx]} {or_end_token} {CNT['answer'][idx]} {end_token}\" \n",
    "        #line=f\"<|user|>\\n{data['question'][idx]}</s>\\n<|assistant|>\\n{data['answer'][idx]}</s>\" \n",
    "        #line=f\"{data['title'][idx]} {data['summary'][idx]} {data['key_fact'][idx]}</s>\" \n",
    "        line=f\"{data['summary'][idx]}</s>\" \n",
    "        #prompt =  f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "    \n",
    "    \n",
    "    \n",
    "        #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "        #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "    \n",
    "        #line=f\"{data['question'][idx]} {data['answer'][idx]}\" \n",
    "        tok=tokenizer(line,  return_tensors=\"pt\",  )\n",
    "    \n",
    "        if tok['input_ids'].shape[1]<max_leng_critical:\n",
    "            \n",
    "            if tok['input_ids'].shape[1]>=max_len:\n",
    "                max_len=tok['input_ids'].shape[1]\n",
    "        \n",
    "                \n",
    "                #print (len (line.split ()))\n",
    "            if len (line.split ())>max_len_txt:\n",
    "                max_len_txt=len (line.split ())\n",
    "        \n",
    "            len_list.append (tok['input_ids'].shape[1])\n",
    "            len_list_txt.append (len (line.split ()))\n",
    "        \n",
    "            text_list.append (line)\n",
    "    print (f\"Max tokenized: {max_len}, max length (words): {max_len_txt}\")\n",
    "    MAX_TOKENIZED_LENGTH=max_len\n",
    "    \n",
    "    \n",
    "if Orca_include:\n",
    "    data=pd.read_csv('ALLDATASETS/train_data_processed_OpenOrca_shorter_train_data_chunked_text.csv_0_Lambda_0_FINAL.csv')\n",
    "    #data.head ()\n",
    "    for idx in tqdm ( range (len (data['title'])) ):\n",
    "        '''\n",
    "        <s>[INST] <<SYS>>\n",
    "        You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "        \n",
    "        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "        <</SYS>>\n",
    "        \n",
    "        There's a llama in my garden 😱 What should I do? [/INST]\n",
    "        '''\n",
    "        system_prompt='You are BioinspiredLLM, a helpful assistant.'\n",
    "        ########################################################\n",
    "        #line=f\"{CNT['text'][idx]}{end_token}\"\n",
    "        #line=f\"{or_start_token} {CNT['question'][idx]} {or_end_token} {CNT['answer'][idx]} {end_token}\" \n",
    "        #line=f\"<|user|>\\n{data['question'][idx]}</s>\\n<|assistant|>\\n{data['answer'][idx]}</s>\" \n",
    "        line=f\"{data['title'][idx]} {data['summary'][idx]} {data['key_fact'][idx]}</s>\" \n",
    "        #prompt =  f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "    \n",
    "    \n",
    "    \n",
    "        #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "        #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "    \n",
    "        #line=f\"{data['question'][idx]} {data['answer'][idx]}\" \n",
    "        tok=tokenizer(line,  return_tensors=\"pt\",  )\n",
    "    \n",
    "        if tok['input_ids'].shape[1]<max_leng_critical:\n",
    "            \n",
    "            if tok['input_ids'].shape[1]>=max_len:\n",
    "                max_len=tok['input_ids'].shape[1]\n",
    "        \n",
    "                \n",
    "                #print (len (line.split ()))\n",
    "            if len (line.split ())>max_len_txt:\n",
    "                max_len_txt=len (line.split ())\n",
    "        \n",
    "            len_list.append (tok['input_ids'].shape[1])\n",
    "            len_list_txt.append (len (line.split ()))\n",
    "        \n",
    "            text_list.append (line)\n",
    "    print (f\"Max tokenized: {max_len}, max length (words): {max_len_txt}\")\n",
    "    MAX_TOKENIZED_LENGTH=max_len\n",
    "    \n",
    "\n",
    "\n",
    "if numerical_include:\n",
    "    \n",
    "    data=pd.read_csv('ALLDATASETS/train_data_processed_GPT35-numerical_train_data_chunked_text-COMBINED.csv')\n",
    "    #data.head ()\n",
    "    for idx in tqdm ( range (len (data['title'])) ):\n",
    "        '''\n",
    "        <s>[INST] <<SYS>>\n",
    "        You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "        \n",
    "        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "        <</SYS>>\n",
    "        \n",
    "        There's a llama in my garden 😱 What should I do? [/INST]\n",
    "        '''\n",
    "        system_prompt='You are BioinspiredLLM, a helpful assistant.'\n",
    "        ########################################################\n",
    "        #line=f\"{CNT['text'][idx]}{end_token}\"\n",
    "        #line=f\"{or_start_token} {CNT['question'][idx]} {or_end_token} {CNT['answer'][idx]} {end_token}\" \n",
    "        #line=f\"<|user|>\\n{data['question'][idx]}</s>\\n<|assistant|>\\n{data['answer'][idx]}</s>\" \n",
    "        \n",
    "        if data['summary'][idx] != '':\n",
    "            line=f\"{data['summary'][idx]}</s>\" \n",
    "            #prompt =  f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "        \n",
    "        \n",
    "        \n",
    "            #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "            #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "        \n",
    "            #line=f\"{data['question'][idx]} {data['answer'][idx]}\" \n",
    "            tok=tokenizer(line,  return_tensors=\"pt\",  )\n",
    "        \n",
    "            if tok['input_ids'].shape[1]<max_leng_critical:\n",
    "                \n",
    "                if tok['input_ids'].shape[1]>=max_len:\n",
    "                    max_len=tok['input_ids'].shape[1]\n",
    "            \n",
    "                    \n",
    "                    #print (len (line.split ()))\n",
    "                if len (line.split ())>max_len_txt:\n",
    "                    max_len_txt=len (line.split ())\n",
    "            \n",
    "                len_list.append (tok['input_ids'].shape[1])\n",
    "                len_list_txt.append (len (line.split ()))\n",
    "            \n",
    "                text_list.append (line)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data=pd.read_csv('ALLDATASETS/train_data_processed_GPT35-numerical_train_data_chunked_text_v3_REPEAT.csv')\n",
    "    #data.head ()\n",
    "    for idx in tqdm ( range (len (data['title'])) ):\n",
    "        '''\n",
    "        <s>[INST] <<SYS>>\n",
    "        You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "        \n",
    "        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "        <</SYS>>\n",
    "        \n",
    "        There's a llama in my garden 😱 What should I do? [/INST]\n",
    "        '''\n",
    "        system_prompt='You are BioinspiredLLM, a helpful assistant.'\n",
    "        ########################################################\n",
    "        #line=f\"{CNT['text'][idx]}{end_token}\"\n",
    "        #line=f\"{or_start_token} {CNT['question'][idx]} {or_end_token} {CNT['answer'][idx]} {end_token}\" \n",
    "        #line=f\"<|user|>\\n{data['question'][idx]}</s>\\n<|assistant|>\\n{data['answer'][idx]}</s>\" \n",
    "        \n",
    "        if data['summary'][idx] != '':\n",
    "            line=f\"{data['summary'][idx]}</s>\" \n",
    "            #prompt =  f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "        \n",
    "        \n",
    "        \n",
    "            #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "            #line = f\"[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{data['question'][idx]}[/INST]{data['answer'][idx]}</s>\"\n",
    "        \n",
    "            #line=f\"{data['question'][idx]} {data['answer'][idx]}\" \n",
    "            tok=tokenizer(line,  return_tensors=\"pt\",  )\n",
    "        \n",
    "            if tok['input_ids'].shape[1]<max_leng_critical:\n",
    "                \n",
    "                if tok['input_ids'].shape[1]>=max_len:\n",
    "                    max_len=tok['input_ids'].shape[1]\n",
    "            \n",
    "                    \n",
    "                    #print (len (line.split ()))\n",
    "                if len (line.split ())>max_len_txt:\n",
    "                    max_len_txt=len (line.split ())\n",
    "            \n",
    "                len_list.append (tok['input_ids'].shape[1])\n",
    "                len_list_txt.append (len (line.split ()))\n",
    "            \n",
    "                text_list.append (line)\n",
    "print (f\"Max tokenized: {max_len}, max length (words): {max_len_txt}\")\n",
    "MAX_TOKENIZED_LENGTH=max_len\n",
    "\n",
    "import random\n",
    "\n",
    " \n",
    "# Shuffle the list\n",
    "random.shuffle(text_list)\n",
    "\n",
    "text_only=False #True\n",
    "#text_only=True #True\n",
    "\n",
    "if text_only:\n",
    "    import datasets \n",
    "    df_out = pd.DataFrame({\"text\": text_list,})\n",
    "    train_dataset = datasets.Dataset.from_pandas(df_out)\n",
    "    \n",
    "else:\n",
    "    train_dataset = Dataset(text_list )\n",
    "\n",
    "\n",
    "\n",
    "n_fragments=len (text_list)\n",
    "len (train_dataset), isinstance(train_dataset, torch.utils.data.Dataset)\n",
    "#MAX_TOKENIZED_LENGTH=768\n",
    "\n",
    "plt.hist (len_list, label = 'Token length', density=True, bins=50)\n",
    "plt.hist (len_list_txt, label = 'Text length (words)', density=True, bins=50)\n",
    "plt.legend ()\n",
    "plt.savefig ('dist.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68007382-6bee-4e8c-8543-38ac499cb265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc6397e2-2c3e-41d4-a22d-3a8083cf15c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TOKENIZED_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7987b65e-4409-464e-8dac-15516effb94a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|user|>\\n Question: What is the dominant fracture path observed in the shell structure being studied in the investigation?</s>\\n<|assistant|>\\n The dominant fracture path observed in the shell structure being studied in the investigation is along lamella boundaries.</s>\\n',\n",
       " '<|user|>\\n What is the difference between the crystallographic orientation of the major structural units in the primary plates and the lamellar needle complex?</s>\\n<|assistant|>\\n The crystallographic orientation of the major structural units in the primary plates and the lamellar needle complex is different. In the primary plates, the major structural units are oriented in two different directions: (a) orange to red at the outermost margin of the primary plates and (b) light green at the inter with the stone. In contrast, the major structural units in the lamellar needle complex are oriented in a different direction, with no specific orientation patterns mentioned.</s>\\n',\n",
       " '<|user|>\\n What were the results of the tensile testing of the scaffolds?</s>\\n<|assistant|>\\n The results of the tensile testing of the scaffolds were not provided in the text.</s>\\n',\n",
       " '<|user|>\\n 1. What are lapillus and asteriscus?</s>\\n<|assistant|>\\n Lapillus and asteriscus are two types of otoliths found in the inner ear of fish. Lapillus is composed of aragonite, while asteriscus is composed of vaterite.</s>\\n',\n",
       " '<|user|>\\n What is the expected outcome of the study in terms of developing an accurate constitutive description of the non-linear dynamic behavior for the machined material?</s>\\n<|assistant|>\\n The expected outcome of the study is to develop an accurate constitutive description of the non-linear dynamic behavior for the machined material in ultra-precision diamond turning (UPDT) of silk fibroin.</s>\\n',\n",
       " '<|user|>\\n Question: What is the equation used to describe the progression of the freezing front in the Stefan problem?</s>\\n<|assistant|>\\n The equation used to describe the progression of the freezing front in the Stefan problem is (6.13): lS rhE s+ ds, dt = lS (TE - T0 ) k.</s>\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4bec9d1-09c8-4a2b-a7b4-ee7266c8a21d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TOKENIZED_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d706c8da-672b-48c1-b72c-2e77412760bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62])\n",
      "tensor([    1,   523, 28766,  1838, 28766, 28767,    13,  1824,   349,   272,\n",
      "         5133,  1444,   272, 26148,   455, 12293, 18159,   302,   272,  3014,\n",
      "        21431,  8007,   297,   272,  6258, 19762,   304,   272,   305,   314,\n",
      "          479,   283, 25710,  4630, 28804,     2, 28705,    13, 28789, 28766,\n",
      "          489, 11143, 28766, 28767,    13,   415, 26148,   455, 12293, 18159,\n",
      "          302,   272,  3014, 21431,  8007,   297,   272,  6258, 19762,   304,\n",
      "          272,   305,   314,   479,   283, 25710,  4630,   349,  1581, 28723,\n",
      "          560,   272,  6258, 19762, 28725,   272,  3014, 21431,  8007,   460,\n",
      "         9093,   286,   297,   989,  1581, 14278, 28747,   325, 28708, 28731,\n",
      "        14545,   298,  2760,   438,   272, 12859,  2284,  9829,   302,   272,\n",
      "         6258, 19762,   304,   325, 28726, 28731,  2061,  5344,   438,   272,\n",
      "          791,   395,   272,  7253, 28723,   560,  9349, 28725,   272,  3014,\n",
      "        21431,  8007,   297,   272,   305,   314,   479,   283, 25710,  4630,\n",
      "          460,  9093,   286,   297,   264,  1581,  5007, 28725,   395,   708,\n",
      "         2948, 18159, 11533,  7083, 28723,     2, 28705,    13])\n",
      "torch.Size([54])\n",
      "tensor([    1,   523, 28766,  1838, 28766, 28767,    13, 28705, 28740, 28723,\n",
      "         1824,   460, 14348,   425,   381,   304, 28693,  8286,   381, 28804,\n",
      "            2, 28705,    13, 28789, 28766,   489, 11143, 28766, 28767,    13,\n",
      "        23760,   425,   381,   304, 28693,  8286,   381,   460,   989,  4514,\n",
      "          302,  6957,   328,   372, 28713,  1419,   297,   272,  7450,  8120,\n",
      "          302,  8006, 28723, 23760,   425,   381,   349, 15021,   302,   597,\n",
      "         4959,   570, 28725,  1312, 28693,  8286,   381,   349, 15021,   302,\n",
      "          363,   795,   570, 28723,     2, 28705,    13])\n",
      "torch.Size([95])\n",
      "tensor([    1,   523, 28766,  1838, 28766, 28767,    13, 22478, 28747,  1824,\n",
      "          349,   272,  8777,  1307,   298,  6685,   272,  5097,   296,   302,\n",
      "          272,  1933, 14508,  2778,   297,   272, 27656,  2700, 28804,     2,\n",
      "        28705,    13, 28789, 28766,   489, 11143, 28766, 28767,    13,   415,\n",
      "         8777,  1307,   298,  6685,   272,  5097,   296,   302,   272,  1933,\n",
      "        14508,  2778,   297,   272, 27656,  2700,   349,   325, 28784, 28723,\n",
      "        28740, 28770,  1329,   305, 28735, 23535, 28749,   268, 28806, 18099,\n",
      "        28725, 16599,   327,   305, 28735,   325,  3392,   387,   320, 28734,\n",
      "         1143,   446, 28723,     2, 28705,    13])\n",
      "torch.Size([187])\n",
      "tensor([    1,   523, 28766,  1838, 28766, 28767,    13,  1824,   349,   272,\n",
      "        16827,  4431,  1307,   297,   272,  3881,   298,  2231,  1830,  2473,\n",
      "          497,   369, 26302,   294,   272,  4693,   302,  3495,  5786, 28804,\n",
      "            2, 28705,    13, 28789, 28766,   489, 11143, 28766, 28767,    13,\n",
      "          415, 16827,  4431,  1307,   297,   272,  3881,   298,  2231,  1830,\n",
      "         2473,   497,   369, 26302,   294,   272,  4693,   302,  3495,  5786,\n",
      "        14657,  2662,  7346, 23581,  1830,  5064, 16744,  4213,   264,  2958,\n",
      "          294,   538, 14742, 28733,  1142,   313,   352, 28733, 11023,  2038,\n",
      "          304,   503,  2031,   288,   305,   372,  5064, 28723,     2, 28705,\n",
      "           13])\n"
     ]
    }
   ],
   "source": [
    "iterator=iter (train_dataset)\n",
    "for i in range (4):# (len (train_dataset)):\n",
    "    #print (next (iterator)['text'] )\n",
    "    print (next (iterator)['input_ids'].shape )\n",
    "    print (next (iterator)['input_ids'] )\n",
    "    #print (next (iterator)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a973572-b93b-483b-83ba-91fe02dbb52d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d55bf08-d4f4-4a7a-8066-ddfa946f6317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#keep norm layers in high accuracy \n",
    "#for name, module in model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d11913e5-b129-47e4-8b89-c4e2ac909795",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "888b9334-3afd-42ec-aa1a-ae953cabc06f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557c06f-2efe-4fb0-aad3-90a116477f30",
   "metadata": {},
   "source": [
    "### Set up training parameters, and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb3fb299-5815-40c4-9f75-59a52413742a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_steps =  int (epochs*  (int (n_fragments*train_test_split )//batch_size )/gradient_accumulation_steps)\n",
    "steps_per_epoch=total_steps//epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa2e8c39-0206-4df5-8dfd-e64aba9b0883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warmup_ratio=0.01\n",
    "warmupsteps=int (total_steps*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bca8787-e88e-4961-a7c4-debc2d3beb13",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110705, 22141, 1107)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_steps, steps_per_epoch, warmupsteps"
   ]
  },
  {
   "cell_type": "raw",
   "id": "becf0f54-a832-4bc2-9906-90e5a2552987",
   "metadata": {
    "tags": []
   },
   "source": [
    "model.push_to_hub(FT_model_name, private=True)  \n",
    "tokenizer.push_to_hub(FT_model_name, private=True) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7202d5ea-54c4-451f-bba4-96413fec290b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"To create a tough and strong soft material using a biomimetic collagen design, what do I do? \"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "generate_response (text_input=prompt,\n",
    "                   num_return_sequences=1,\n",
    "                                     temperature=.3,max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2530a20-b920-4f54-b861-0dc32fba8ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "590ea5ff-39a8-4f18-bf64-c675e91abefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class push_callback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "\n",
    "        try:\n",
    "            print (\"Step: Pushing model...\", state.global_step )\n",
    "            model.push_to_hub(FT_model_name, private=True)  \n",
    "            \n",
    "        except:\n",
    "            print (\"Push error...\", state.global_step )\n",
    "        #tokenizer.push_to_hub(FT_model_name, private=True)  \n",
    "\n",
    "        #try:\n",
    "        #    model.save_pretrained (output_dir+\"/checkpoint\")\n",
    "\n",
    "        try:\n",
    "            df=pd.DataFrame(trainer.state.log_history)\n",
    "            df.to_csv(output_dir+loss_file)\n",
    "    \n",
    "            plt.plot (df['step'], df['loss'], 'o-', label = 'Training loss', )\n",
    "            plt.legend ()\n",
    "            plt.ylabel ('Loss')\n",
    "            plt.xlabel  ('Step')\n",
    "            plt.savefig(output_dir+f'/loss_{state.global_step}.svg')\n",
    "            plt.show()\n",
    "        except:\n",
    "            print (\"Plotting failed.\")\n",
    "\n",
    "        try:\n",
    "            #result=generate_response (text_input=f\"{start_token}[INST]To create a tough and strong soft material using a biomimetic collagen design, what do I do? [/INST]\",num_return_sequences=1,\n",
    "            #                         temperature=.5,max_new_tokens=128)\n",
    "\n",
    "            #trial ()\n",
    "            df_res=trial (df_trial,step_number=state.global_step, frac= 1)\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print (\"Trial generation failed.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86156a78-c09b-4fc3-bcd5-3b77f66daa7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ba4065308048be8b7b341c6049a649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################################\n",
      "torch.Size([1, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1547: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When describing gradient structures, they can be gradients in? A) dimension B) composition C) both dimension and composition\n",
      "Answer: C) both dimension and composition is the correct answer when describing gradient structures. Gradient structures can exhibit gradients in both dimension and composition. This means that the properties of the material, such as composition, crystal structure, or magnetic properties, can vary smoothly and gradually in both spatial and compositional directions. Gradient structures are commonly used in materials science and engineering to tailor the properties of materials for specific applications.\n",
      "Correct answer=C\n",
      "###########################################################################\n",
      "torch.Size([1, 63])\n"
     ]
    }
   ],
   "source": [
    "df_res=trial (df_trial,step_number=223, frac= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa26b4b-b9fd-4f22-885a-8368f21c51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+\"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad4c27-6557-4cc9-96de-9fe2cd0799ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = output_dir\n",
    "per_device_train_batch_size = batch_size\n",
    "gradient_accumulation_steps = gradient_accumulation_steps\n",
    "#optim = \"paged_adamw_32bit\"\n",
    "optim = \"paged_adamw_8bit\"\n",
    "save_steps = steps_per_epoch\n",
    "logging_steps = steps_per_epoch\n",
    "learning_rate = learning_rate\n",
    "max_grad_norm = 0.3\n",
    "max_steps = total_steps\n",
    " \n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    #fp16=True,\n",
    "    bf16=True, #on A100\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    #group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    save_total_limit=50,\n",
    "    report_to= \"none\", #change if you want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e986c-1caf-47d6-b765-f1ebafcd1933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " FT_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3ff78-5d58-4e73-a8ff-b4aea9a461cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180cafb-3714-4330-88d0-7a55e2fbff88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import   DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "#max_seq_length = MAX_TOKENIZED_LENGTH\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data, #dataset, #train_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "'''\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        args=training_arguments,\n",
    "        #data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "   data_collator=DataCollatorForSeq2Seq(tokenizer, return_tensors='pt', padding=True),\n",
    "    callbacks=[push_callback],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eded98-56c3-4e01-811a-e3a4e70d2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "len (train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da85e5-2375-4dec-9212-55273c029629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8293d94-ed59-415d-8a72-df4c49ec1f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for name, module in model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bf230-1617-4d90-be8b-0581e6b65ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_res=trial (df_trial,step_number=223, frac= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a70dc-569c-49ef-9d3c-59108623e63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317be1c-3a45-4c91-97c8-9a4e75c1dcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef7185-3147-4904-8494-2610ed3a3109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint = f'./{FT_model_name}/tmp-checkpoint-15390//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c75eb5-b731-4ce6-b76b-7934c1b970ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee38a9c-8ac2-4bc1-9a63-dba2a62e63c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92230a73-d57d-47ca-997f-0d77d5adeb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
